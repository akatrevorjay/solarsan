resource {{ res.name }} {
    protocol {{ res.protocol|default("C") }};

    handlers {
        pri-on-incon-degr "/usr/lib/drbd/notify-pri-on-incon-degr.sh root";
        pri-lost-after-sb "/usr/lib/drbd/notify-pri-lost-after-sb.sh root";
        pri-lost "/usr/lib/drbd/notify-pri-lost.sh root";
        #fence-peer "/usr/lib/drbd/outdate-peer.sh";
        local-io-error "/usr/lib/drbd/notify-io-error.sh root";
        #initial-split-brain "/usr/lib/drbd/notify-split-brain.sh root";
        split-brain "/usr/lib/drbd/notify-split-brain.sh root";
        #before-resync-target "/usr/lib/drbd/notify.sh root";
        #after-resync-target "/usr/lib/drbd/notify.sh root";
    }

    startup {
        wfc-timeout 15;  # 15 -- 0
        degr-wfc-timeout 60;  # 60 -- 120

        # Automatically bring up this hostname as primary in init script
        #become-primary-on $hostname
    }

    disk {
        on-io-error detach;  # or panic

        # If node becomes a disconnected Primary (default is dont-care)
        # Call fence-peer handler, which is supposed to reach out to the other node and 'drbdadm outdate-res $res'
        #fencing resource-only;
        # Freeze all IO operations, call fence-peer handler, which is supposed to run call 'drbdadm outdate-res $res'
        # If that fails, it should STONITH the peer. IO is resumed as soon as the situation is resolved, or
        # via 'drbdadm resume-io $res'
        #fencing resource-only-and-stonith;

        # Read balancing across peers
        # (default=prefer-local, prefer-remote, round-robin, least-pending, when-congested-remote, 32K-striping, up to 1M-striping)
        read-balancing 128K-striping;

        # This option helps to solve the cases when the outcome of the resync decision is incompatible with the current
        # role assignment in the cluster.
        # (disconnect, violently,
        #  call-pri-lost (call pri-lost helper on one of the machines, it's supposed to reboot the machine to make it secondary)
        #rr-conflict

        # Fixed IO bandwidth limit for synchronizing (default is variable)
        #resync-rate 100M;

        #disk-barrier yes;

        # Danger! Do not do this!
        #disk-flushes no;
        #md-flushes no;
    }

    net {
        cram-hmac-alg sha1;
        shared-secret "{{ res.shared_secret|default("secret") }}";

        # Replication traffic integrity checking
        #data-integrity-alg sha1;

        # Checksum-based synchronization
        csums-alg sha1;

        # Split brain detected, but at this time there is no Primary
        after-sb-0pri discard-least-changes;
        #after-sb-0pri discard-zero-changes;
        # Split brain detected, but at this time there is one Primary
        after-sb-1pri discard-secondary;
        # Split brain detected, but there are two Primarys (oh lawdy)
        after-sb-2pri disconnect;
    }

    syncer {
        rate {% if res.sync_rate %}{{ res.sync_rate }}{% else %}200m{% endif %};
    }

    # Common peer options
    meta-disk internal;

    {% for peer in [res.local, res.remote] %}
    on {{ peer.hostname }} {
        device /dev/drbd{{ peer.minor }};
        disk {{ peer.device }};
        address {{ peer.address }}:{{ peer.port }};
    }
    {% endfor %}
}
