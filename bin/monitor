#!/usr/bin/env python

from solarsan.core import logger
#from solarsan.utils.cache import cached_property
#from solarsan import conf
#from solarsan.exceptions import FormattedException
from setproctitle import setproctitle
from circuits import Component, Event, Debugger, Timer, handler
#from circuits.tools import inspect
from storage.drbd import DrbdResource, drbd_overview_parser
from cluster.models import Peer
#from target.models import iSCSITarget
import target.scst
from solarsan.monitor.udev import UDev
from solarsan.monitor.discovery import Discovery


"""
Monitor
"""


class MonitorStatusUpdate(Event):
    """Monitor Status Update"""


class Monitor(Component):
    def __init__(self):
        super(Monitor, self).__init__()
        logger.info('Monitor starting..')

        Discovery().register(self)
        PeerManager().register(self)
        ResourceManager().register(self)
        TargetManager().register(self)
        DeviceManager().register(self)

    def started(self, *args):
        self.fire(MonitorStatusUpdate('Started'))
        logger.info('Monitor started.')

    @handler('monitor_status_update', channel='*')
    def status_update(self, append=None):
        set_proctitle(append)

    #def peer_heartbeat(self, *args, **kwargs):
    #    logger.info('Peer Online @ Monitor')


"""
Peer Manager
"""


class PeerHeartbeat(Event):
    """Remote PeerHeartbeat"""
    #complete = True


class PeerHeartbeatTimeout(Event):
    """Remote PeerHeartbeat Timeout"""
    #complete = True


class PeerPoolHealthCheck(Event):
    """Checks Pool Health"""


class PeerManager(Component):
    heartbeat_every = 5.0
    pool_health_every = 10.0

    def __init__(self):
        super(PeerManager, self).__init__()
        self.peers = {}
        self.monitors = {}
        Timer(self.heartbeat_every, PeerHeartbeat(), persist=True).register(self)
        Timer(self.pool_health_every, PeerPoolHealthCheck(), persist=True).register(self)

    @handler('started', channel='*')
    def _on_started(self, *args):
        for peer in Peer.objects.all():
            self.add_peer(peer)

    @handler('peer_discovered', channel='*')
    def _on_peer_discovered(self, peer, created=None):
        self.add_peer(peer)

    def add_peer(self, peer):
        if peer.hostname in self.peers:
            return
        logger.info("Monitoring Peer '%s'.", peer.hostname)
        self.peers[peer.hostname] = peer
        self.monitors[peer.hostname] = PeerMonitor(peer).register(self)


"""
Resource Manager
"""


class ResourceHealthCheck(Event):
    """Check Resource Health"""


class ResourceManager(Component):
    res_health_check_every = 10.0

    def __init__(self):
        super(ResourceManager, self).__init__()
        self.resources = {}
        self.monitors = {}
        Timer(self.res_health_check_every, ResourceHealthCheck(), persist=True).register(self)

    @handler('started', channel='*')
    def _on_started(self, *args):
        self.do_all_resources()

    def do_all_resources(self):
        for res in DrbdResource.objects.all():
            self.add_res(res)

    def add_res(self, res):
        if res.name in self.resources:
            return
        logger.info("Monitoring Resource '%s'.", res.name)
        self.resources[res.name] = res
        self.monitors[res.name] = ResourceMonitor(res).register(self)


"""
Target Manager
"""

"""
On Startup:
    Add each target manually

"""
import sh


class TargetWriteConfig(Event):
    """Write Target Config"""


class TargetClearConfig(Event):
    """Clear Target Config"""


class TargetReloadConfig(Event):
    """Write Target Config"""


class TargetManager(Component):

    def __init__(self):
        super(TargetManager, self).__init__()
        self.res_skip = {}

    @handler('target_write_config', channel='*')
    def _on_target_write_config(self):
        target.scst.write_config(skip=self.res_skip.keys())
        self.fire(TargetReloadConfig())

    @handler('target_reload_config', channel='*')
    def _on_target_reload_config(self):
        target.scst.reload_config(force=True)

    @handler('target_clear_config', channel='*')
    def _on_target_clear_config(self):
        target.scst.clear_config(force=True)

    #def resource_role_change(self, res, role):
    #    if role in ['Primary', 'Secondary']:
    #        self.fire(TargetWriteConfig())

    def resource_primary_post(self, res):
        self.fire(TargetWriteConfig())

    def resource_secondary_pre(self, res):
        self.res_skip[res.name] = True

        # TODO Find a way to exclude res
        self.fire(TargetWriteConfig(skip=res.name))


"""
Device Manager
"""


class DeviceManager(Component):
    def __init__(self):
        super(DeviceManager, self).__init__()

        UDev().register(self)

    def device_add(self, device):
        pass

    def device_remove(self, device):
        pass


"""
Peer Monitor
"""


class PeerOnline(Event):
    """Peer Online Event"""


class PeerOffline(Event):
    """Peer Offline Event"""


class PeerPoolNotHealthy(Event):
    """Pool is Not Healthy"""


class PeerMonitor(Component):
    heartbeat_timeout_after = 2

    def __init__(self, peer):
        super(PeerMonitor, self).__init__()
        self.peer = peer
        self._heartbeat_timeout_count = None

        if self.peer.is_offline:
            logger.warning('Peer "%s" is already marked as offline. Marking online to ensure this is ' +
                           'still true.', self.peer.hostname)
            self.mark_online(startup=True)

    @handler('peer_heartbeat', channel='*')
    def _on_peer_heartbeat(self):
        # This is done so the first try is always attempted, even if the Peer
        # is offline.
        if not self._heartbeat_timeout_count:
            self._heartbeat_timeout_count = 0
        elif self.peer.is_offline:
            #logger.debug('PeerHeartbeat is not even being attempted as Peer "%s" is marked offline.', self.peer.hostname)
            return

        ret = self.ping()
        if ret:
            self._heartbeat_timeout_count = 0
        else:
            self._heartbeat_timeout_count += 1
            if self._heartbeat_timeout_count >= self.heartbeat_timeout_after:
                self.fire(PeerOffline(self.peer))
        return ret

    def ping(self):
        try:
            storage = self.peer.get_service('storage', default=None)
            storage.ping()
            #logger.debug('PeerHeartbeat back from Peer "%s"', self.peer.hostname)
            return True
        except:
            logger.error('Did not receive heartbeat for Peer "%s"', self.peer.hostname)
            return False

    @handler('peer_online', channel='*')
    def _on_peer_online(self, peer):
        if peer.hostname != self.peer.hostname:
            return
        if self.peer.is_offline:
            self.mark_online()

    @handler('peer_offline', channel='*')
    def _on_peer_offline(self, peer):
        if peer.hostname != self.peer.hostname:
            return
        if not self.peer.is_offline:
            self.mark_offline()

    def mark_online(self, startup=False):
        if not startup:
            logger.warning('Peer "%s" is ONLINE!', self.peer.hostname)
        self.peer.is_offline = False
        self.peer.save()
        self._heartbeat_timeout_count = 0

    def mark_offline(self):
        logger.error('Peer "%s" is OFFLINE!', self.peer.hostname)
        self.peer.is_offline = True
        self.peer.save()

    @handler('peer_discovered', channel='*')
    def _on_peer_discovered(self, peer, created=False):
        if peer.hostname != self.peer.hostname:
            return
        if self.peer.is_offline:
            self.fire(PeerOnline(self.peer))

    @handler('peer_pool_health_check', channel='*')
    def _on_peer_pool_health_check(self):
        if self.peer.is_offline:
            return
        try:
            storage = self.peer.get_service('storage', default=None)
            Pool = storage.root.pool()
        except AttributeError:
            logger.error('Could not connect to Peer "%s". Is it offline and we don\'t know it yet?',
                         self.peer.hostname)
            return
        pools = Pool.list()
        #logger.debug('Checking health of Pools "%s" on Peer "%s"', pools, self.peer.hostname)
        ret = True
        for pool in pools:
            if not pool.is_healthy():
                self.fire(PeerPoolNotHealthy(self.peer, pool.name))
                ret = False
        return ret

    @handler('peer_pool_not_healthy', channel='*')
    def _on_peer_pool_not_healthy(self, peer, pool):
        if peer.hostname != self.peer.hostname:
            return
        logger.error('Pool "%s" on Peer "%s" is NOT healthy!', pool, self.peer.hostname)

        # TODO If Pool has remote disks, and those are the only ones that are
        # missing, try a 'zpool clear'


"""
Resource Monitor
"""


class ResourcePrimaryPre(Event):
    """Promote to Primary"""


class ResourcePrimary(Event):
    """Promote to Primary"""


class ResourcePrimaryPost(Event):
    """Promote to Primary"""


class ResourceSecondaryPre(Event):
    """Demote to Secondary"""


class ResourceSecondary(Event):
    """Demote to Secondary"""


class ResourceSecondaryPost(Event):
    """Demote to Secondary"""


class ResourceConnectionStateChange(Event):
    """Resource Connection State Change"""


class ResourceDiskStateChange(Event):
    """Resource Disk State Change"""


class ResourceRoleChange(Event):
    """Resource Connection State Change"""
    #success = True
    #complete = True


class ResourceRemoteDiskStateChange(Event):
    """Resource Remote Disk State Change"""


class ResourceRemoteRoleChange(Event):
    """Resource Remote Role State Change"""


class ResourceMonitor(Component):
    def __init__(self, res):
        super(ResourceMonitor, self).__init__()
        self.res = res
        self.status()

    @handler('peer_offline', channel='*')
    def _on_peer_offline(self, peer):
        if peer.hostname != self.res.remote.hostname:
            return
        logger.error('Remote "%s" for Resource "%s" went offline.',
                     self.res.remote.hostname, self.res.name)

        ## TODO Take over the volumes, write out any targets, enable any HA ips, etc
        #if self.res.role != 'Primary':
        #    self.fire(ResourcePrimary(self.res))

    @handler('peer_pool_not_healthy', channel='*')
    def _on_peer_pool_not_healthy(self, peer, pool):
        if peer.hostname == self.res.local.hostname and pool == self.res.local.pool:
            #logger.error('Pool "%s" for Resource "%s" is not healthy.',
            #             pool, self.res.name)
            pass

            ## TODO Remove any HA IPs for this resource (handled by the target,
            # nothing here)
            #   - But what if the HA IPs are used for others? HA IPs must be
            #   attached to targets, and only whole targets can go up or down.
            # TODO Remove any targets for this resource
            #   - But what if the targets are used for other resources? Again,
            #   must do a whole target at a time, up or down.
            # TODO Secondary ourselves
            #self.fire(ResourceSecondary(self.res))

        elif peer.hostname == self.res.remote.hostname and pool == self.res.remote.pool:
            #logger.error('Pool "%s" on Remote "%s" for Resource "%s" is not healthy.',
            #             pool, self.res.remote.hostname, self.res.name)
            pass

            # TODO Primary ourselves
            # TODO Add any targets for this resource
            #   - But what if the targets are used for other resources? Again,
            #   must do a whole target at a time, up or down.
            # TODO Add any HA IPs for that target (handled by the target,
            # nothing here)
            #   - But what if the HA IPs are used for others? HA IPs must be
            #   attached to targets, and only whole targets can go up or down.
            #self.fire(ResourcePrimary(self.res))

    @property
    def service(self):
        return self.res.local.service

    #def get_service(self):
    #    return self.res.local.service

    """
    Status Tracking
    """

    def status(self):
        ret = drbd_overview_parser(self.res.name)
        if ret:
            if ret['connection_state'] != self.res.connection_state:
                self.fire(ResourceConnectionStateChange(self.res, ret['connection_state']))

            if ret['disk_state'] != self.res.disk_state:
                self.fire(ResourceDiskStateChange(self.res, ret['disk_state']))
            if ret['role'] != self.res.role:
                self.fire(ResourceRoleChange(self.res, ret['role']))

            if ret['remote_disk_state'] != self.res.remote_disk_state:
                self.fire(ResourceRemoteDiskStateChange(self.res, ret['remote_disk_state']))
            if ret['remote_role'] != self.res.remote_role:
                self.fire(ResourceRemoteRoleChange(self.res, ret['remote_role']))
        return ret

    def resource_connection_state_change(self, res, cstate):
        if self.res.name != res.name:
            return

        if cstate == 'Connected':
            """Connected. A DRBD connection has been established, data mirroring is now active. This is the
            normal state"""
            logger.error("Resource '%s' is %s!", self.res.name, cstate)

        elif cstate == 'WFConnection':
            """WFConnection. This node is waiting until the peer node becomes visible on the network."""
            logger.error("Resource '%s' is %s!", self.res.name, cstate)
            #if self.res.role != 'Primary':
            #    self.fire(ResourcePrimary(self.res))

        elif cstate == 'StandAlone':
            """StandAlone. No network configuration available. The resource has not yet been connected, or
            has been administratively disconnected (using drbdadm disconnect), or has dropped its connection
            due to failed authentication or split brain."""
            logger.error("Resource '%s' is %s!", self.res.name, cstate)
            #if self.res.role != 'Primary':
            #    self.fire(ResourcePrimary(self.res))

        elif cstate == 'SyncSource':
            """SyncSource. Synchronization is currently running, with the local node being the source of
            synchronization"""
            logger.error("Resource '%s' is %s!", self.res.name, cstate)
            #if self.res.role != 'Primary':
            #    self.fire(ResourcePrimary(self.res))

        elif cstate == 'PausedSyncS':
            """PausedSyncS. The local node is the source of an ongoing synchronization, but synchronization
            is currently paused. This may be due to a dependency on the completion of another synchronization
            process, or due to synchronization having been manually interrupted by drbdadm pause-sync."""
            logger.error("Resource '%s' is %s!", self.res.name, cstate)

        elif cstate == 'SyncTarget':
            """SyncTarget. Synchronization is currently running, with the local node being the target of
            synchronization."""
            logger.error("Resource '%s' is %s!", self.res.name, cstate)

        elif cstate == 'PausedSyncT':
            """PausedSyncT. The local node is the target of an ongoing synchronization, but synchronization
            is currently paused. This may be due to a dependency on the completion of another synchronization
            process, or due to synchronization having been manually interrupted by drbdadm pause-sync."""
            logger.error("Resource '%s' is %s!", self.res.name, cstate)

        elif cstate == 'VerifyS':
            """VerifyS. On-line device verification is currently running, with the local node being the source
            of verification."""
            logger.error("Resource '%s' is %s!", self.res.name, cstate)

        elif cstate == 'VerifyT':
            """VerifyT. On-line device verification is currently running, with the local node being the target
            of verification."""
            logger.error("Resource '%s' is %s!", self.res.name, cstate)

        else:
            raise Exception('Resource "%s" has an unknown connection state of "%s"!', cstate)

        self.res.connection_state = cstate
        self.res.save()

    def resource_disk_state_change(self, res, dstate):
        if self.res.name != res.name:
            return

        if dstate == 'UpToDate':
            """UpToDate. Consistent, up-to-date state of the data. This is the normal state."""
            logger.error("Resource '%s' is %s!", self.res.name, dstate)

        elif dstate == 'Diskless':
            """Diskless. No local block device has been assigned to the DRBD driver. This may mean that the
            resource has never attached to its backing device, that it has been manually detached using
            drbdadm detach, or that it automatically detached after a lower-level I/O error."""
            logger.error("Resource '%s' is %s!", self.res.name, dstate)

        elif dstate == 'Inconsistent':
            """Inconsistent. The data is inconsistent. This status occurs immediately upon creation of a new
            resource, on both nodes (before the initial full sync). Also, this status is found in one node
            (the synchronization target) during synchronization."""
            logger.error("Resource '%s' is %s!", self.res.name, dstate)

        elif dstate == 'Outdated':
            """Outdated. Resource data is consistent, but outdated."""
            logger.error("Resource '%s' is %s!", self.res.name, dstate)

        elif dstate == 'DUnknown':
            """DUnknown. This state is used for the peer disk if no network connection is available."""
            logger.error("Resource '%s' is %s!", self.res.name, dstate)

        elif dstate == 'Consistent':
            """Consistent. Consistent data of a node without connection. When the connection is established,
            it is decided whether the data is UpToDate or Outdated."""
            logger.error("Resource '%s' is %s!", self.res.name, dstate)

        else:
            raise Exception('Resource "%s" has an unknown disk state of "%s"!', dstate)

        self.res.disk_state = dstate
        self.res.save()

    def resource_role_change(self, res, role):
        if self.res.name != res.name:
            return

        if role == 'Primary':
            """Primary. The resource is currently in the primary role, and may be read from and written to.
            This role only occurs on one of the two nodes, unless dual-primary mode is enabled."""
            logger.error("Resource '%s' is %s!", self.res.name, role)

        elif role == 'Secondary':
            """Secondary. The resource is currently in the secondary role. It normally receives updates from
            its peer (unless running in disconnected mode), but may neither be read from nor written to.
            This role may occur on one or both nodes."""
            logger.error("Resource '%s' is %s!", self.res.name, role)

        elif role == 'Unknown':
            """Unknown. The resource's role is currently unknown. The local resource role never has this
            status. It is only displayed for the peer's resource role, and only in disconnected mode."""
            logger.error("Resource '%s' is %s!", self.res.name, role)

        else:
            raise Exception('Resource "%s" has an unknown role of "%s"!', role)

        self.res.role = role
        self.res.save()

    def resource_remote_disk_state_change(self, res, dstate):
        if self.res.name != res.name:
            return
        self.res.remote_disk_state = dstate
        self.res.save()

    def resource_remote_role_change(self, res, remote_role):
        if self.res.name != res.name:
            return
        self.res.remote_role = remote_role
        self.res.save()

    """
    Health Check
    """

    def resource_health_check(self):
        self.status()

        ## Check disk state
        #if self.res.disk_state != 'UpToDate':
        #    logger.error('Resource "%s" has a disk state of "%s", not "UpToDate"!',
        #                 self.res.name, self.res.disk_state)

        #if self.res.remote_disk_state != 'UpToDate':
        #    logger.error('Resource "%s" on Remote "%s" has a disk state of "%s", not "UpToDate"!',
        #                 self.res.name, self.res.remote.hostname, self.res.remote_disk_state)
        #    # TODO If remote disk state is not Unknown, take over as Primary
        #    #if self.res.remote_disk_state != 'DUnknown':
        #    #    self.fire(ResourcePrimary(self.res))

        ## Check Connection state
        #if self.res.connection_state != 'Connected':
        #    logger.error('Resource "%s" has a connection state of "%s", not "Connected"!',
        #                 self.res.name, self.res.connection_state)
        #    # TODO If WFConnection, take over as Primary, but doesn't the above
        #    # peer_offline handle that?
        #    #if self.res.connection_state == 'WFConnection':
        #    #    self.fire(ResourcePrimary(self.res))

        # If we have two secondary nodes become primary
        if self.res.role == 'Secondary' and self.res.remote_role == 'Secondary':
            logger.info('Taking over as Primary for Resource "%s" because someone has to. ' +
                        '(dual secondaries).', self.res.name)
            self.fire(ResourcePrimary(self.res))

    """
    Role Switching
    """

    @handler('resource_primary', channel='*')
    def _on_res_primary(self, res):
        if res.name != self.res.name:
            return
        self.status()
        if self.res.role == 'Primary':
            return

        logger.warning('Promoting self to Primary for Resource "%s".', self.res.name)

        if self.res.disk_state != 'UpToDate':
            logger.error('Cannot promote self to Primary for Resource "%s", ' +
                         'as disk state is not UpToDate.', self.res.name)
            return

        self.primary()

    @handler('resource_secondary_success', channel='*')
    def _on_res_secondary(self, res, *values):
        if res.name != self.res.name:
            return
        self.status()
        if self.res.role == 'Secondary':
            return
        logger.warning('Demoting self to Secondary for Resource "%s".', self.res.name)
        self.secondary()

    def primary(self):
        self.fire(ResourcePrimaryPre(self.res))
        self.service.primary()
        self.status()
        self.fire(ResourcePrimaryPost(self.res))

    def secondary(self):
        self.fire(ResourceSecondaryPre(self.res))
        self.service.secondary()
        self.status()
        self.fire(ResourceSecondaryPost(self.res))

    """
    TODO
    """

    #def get_primary(self):
    #    if self._status['role'] == 'Primary':
    #        return self.res.local
    #    elif self._status['remote_role'] == 'Primary':
    #        return self.res.remote

    def write_config(self, adjust=True):
        self.service.write_config()
        if adjust:
            self.service.adjust()
        return True


"""
Main
"""


def set_proctitle(append=None):
    title = 'SolarSan Monitor'
    if append:
        title += ': %s' % append
    setproctitle('[%s]' % title)


def main():
    set_proctitle('Starting')
    try:
        #(Discovery()).run()
        #(Discovery() + Debugger()).run()
        (Monitor() + Debugger()).run()
    except (SystemExit, KeyboardInterrupt):
        raise


if __name__ == '__main__':
    main()
