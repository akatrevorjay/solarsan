#!/usr/bin/env python

from solarsan.core import logger
#from solarsan import conf
#from solarsan.exceptions import FormattedException
from setproctitle import setproctitle
from datetime import datetime
from circuits import Component, Event, Debugger, Timer, handler
#from circuits.tools import inspect
from storage.drbd import DrbdResource
from cluster.models import Peer
#from target.models import iSCSITarget
#import target.scst
import rpyc


"""
Events
"""


"""
Monitor
"""


class MonitorStatusUpdate(Event):
    """Monitor Status Update"""


class Monitor(Component):
    def __init__(self):
        super(Monitor, self).__init__()
        logger.info('Monitor starting..')

        Discovery().register(self)
        PeerManager().register(self)

    def started(self, *args):
        self.fire(MonitorStatusUpdate('Started'))
        logger.info('Monitor started.')

    @handler('monitor_status_update', channel='*')
    def status_update(self, append=None):
        set_proctitle(append)

    #def peer_heartbeat(self, *args, **kwargs):
    #    logger.info('Peer Online @ Monitor')


"""
Peer Manager
"""


class PeerHeartbeat(Event):
    """Remote PeerHeartbeat"""
    #complete = True


class PeerHeartbeatTimeout(Event):
    """Remote PeerHeartbeat Timeout"""
    #complete = True


class PoolHealthCheck(Event):
    """Checks Pool Health"""


class ResourceHealthCheck(Event):
    """Check Resource Health"""


class PeerManager(Component):
    heartbeat_every = 5.0

    def __init__(self):
        super(PeerManager, self).__init__()
        self.peers = {}
        Timer(self.heartbeat_every, PeerHeartbeat(), persist=True).register(self)

    def started(self, *args):
        for peer in Peer.objects.all():
            self.add_peer(peer)

    def peer_discovered(self, peer, created=None):
        self.add_peer(peer)

    def add_peer(self, peer):
        if peer.hostname in self.peers:
            return
        logger.info("Monitoring Peer '%s'.", peer.hostname)
        PeerMonitor(peer).register(self)
        self.peers[peer.hostname] = peer


"""
Discovery
"""


class DiscoverPeers(Event):
    """Discover Event"""
    complete = True


class ProbePeer(Event):
    """Peer Probe Event"""


class PeerDiscovered(Event):
    """Peer Discovered Event"""


class Discovery(Component):
    discover_every = 60.0

    def __init__(self):
        super(Discovery, self).__init__()

    def started(self, *args):
        self.fire(DiscoverPeers())
        Timer(self.discover_every, DiscoverPeers(), persist=True).register(self)

    """
    Discovery
    """

    def discover_peers(self):
        logger.debug("Discovering nearby peers..")
        try:
            for host, port in rpyc.discover('storage'):
                self.fire(ProbePeer(host))
        except Exception:
            pass

    def probe_peer(self, host):
        """Probes a discovered node for info"""

        hostname = None
        ifaces = None
        addrs = None
        cluster_iface = None

        try:
            c = rpyc.connect_by_service('storage', host=host)

            hostname = c.root.peer_hostname()
            cluster_iface = c.root.peer_get_cluster_iface()

            ifaces = c.root.peer_list_addrs()
            addrs = dict([(y['addr'], y['netmask']) for x in ifaces.values() for y in x])

            if None in [hostname, ifaces, addrs, cluster_iface]:
                raise Exception("Peer discovery probe has invalid data.")

            #logger.info("Peer discovery (host='%s'): Hostname is '%s'.", host, hostname)
        except Exception, e:
            #raise FormattedException("Peer discovery (host='%s') failed: %s", host, e)
            logger.error("Peer discovery (host='%s') failed: %s", host, e)
            return

        # TODO Each node should prolly get a UUID, glusterfs already assigns one, but maybe we
        # should do it a layer above.

        peer, created = Peer.objects.get_or_create(hostname=hostname)

        peer.addrs = list(addrs.keys())
        peer.netmasks = list(addrs.values())
        peer.ifaces = list(ifaces.keys())

        peer.cluster_addr = cluster_iface['ipaddr']
        peer.cluster_iface = cluster_iface['name']

        peer.last_seen = datetime.utcnow()

        peer.save()

        self.fire(PeerDiscovered(peer, created=created))
        return True


"""
Peer Monitor
"""


class PeerOnline(Event):
    """Peer Online Event"""


class PeerOffline(Event):
    """Peer Offline Event"""


class ReconnectPeer(Event):
    """Peer reconnection attempt Event"""


class PromoteToPrimary(Event):
    """Promote Resource to Primary"""


class DemoteToSecondary(Event):
    """Demote Resource to Secondary"""


class ResourcePrimary(Event):
    """Takeover as Primary"""


class ResourceSecondary(Event):
    """Takeover as Primary"""


class PeerMonitor(Component):
    heartbeat_timeout_after = 2

    def __init__(self, peer):
        super(PeerMonitor, self).__init__()
        self.peer = peer
        self._heartbeat_timeout_count = None

    @handler('peer_heartbeat', channel='*')
    def _on_peer_heartbeat(self):
        # This is done so the first try is always attempted, even if the Peer
        # is offline.
        if not self._heartbeat_timeout_count:
            self._heartbeat_timeout_count = 0
        elif self.peer.is_offline:
            logger.debug('PeerHeartbeat is not even being attempted as Peer "%s" is marked offline.', self.peer.hostname)
            return

        ret = self.ping()
        if ret:
            self._heartbeat_timeout_count = 0
        else:
            self._heartbeat_timeout_count += 1
            if self._heartbeat_timeout_count >= self.heartbeat_timeout_after:
                self.fire(PeerOffline(self.peer))
        return ret

    def ping(self):
        try:
            storage = self.peer.get_service('storage', default=None)
            storage.ping()
            logger.debug('PeerHeartbeat back from Peer "%s"', self.peer.hostname)
            return True
        except:
            logger.error('Did not receive heartbeat for Peer "%s"', self.peer.hostname)
            return False

    @handler('peer_online', channel='*')
    def _on_peer_online(self, peer):
        if peer.hostname != self.peer.hostname:
            return
        if self.peer.is_offline:
            self.mark_online()

    @handler('peer_offline', channel='*')
    def _on_peer_offline(self, peer):
        if peer.hostname != self.peer.hostname:
            return
        if not self.peer.is_offline:
            self.mark_offline()

    def mark_online(self):
        logger.warning('Peer "%s" is ONLINE!', self.peer.hostname)
        self.peer.is_offline = False
        self.peer.save()
        self._heartbeat_timeout_count = 0

    def mark_offline(self):
        logger.error('Peer "%s" is OFFLINE!', self.peer.hostname)
        self.peer.is_offline = True
        self.peer.save()

        # Take over the volumes, write out any targets, enable any HA ips, etc
        #self.fire(PromoteToPrimary(self.peer))

    @handler('peer_discovered', channel='*')
    def _on_peer_discovered(self, peer, created=False):
        if peer.hostname != self.peer.hostname:
            return
        if self.peer.is_offline:
            self.fire(PeerOnline(self.peer))

    @handler('promote_to_primary', channel='*')
    def _on_promote_to_primary(self, peer):
        if peer.hostname != self.peer.hostname:
            return
        self.primary()

    def primary(self):
        logger.info('Taking over as Primary: all Resources with Peer "%s"',
                    self.remote.hostname)

        for res in self.resources:
            sl = res.local.service
            if not sl.is_primary:
                sl.primary()

        # TODO WRITE OUT ANY TARGETS, SCST CONFIG AS APPLIES

        # TODO ENABLE ANY HA IPS

        logger.info('Done taking over as Primary: all all Resources with Peer "%s"',
                    self.remote.hostname)

    @handler('demote_to_secondary', channel='*')
    def _on_demote_to_secondary(self, peer):
        if peer.hostname != self.peer.hostname:
            return
        self.secondary()

    def secondary(self):
        logger.info('Demoting myself to Secondary: all Resources with Peer "%s"',
                    self.remote.hostname)

        for res in self.resources:
            sl = res.local.service
            if not sl.is_secondary:
                sl.secondary()

        # TODO WRITE OUT ANY TARGETS, SCST CONFIG AS APPLIES

        # TODO ENABLE ANY HA IPS

        logger.info('Done demoting myself to Secondary: all Resources with Peer "%s"',
                    self.remote.hostname)

    @handler('pool_health_check', channel='*')
    def _on_pool_health_check(self):
        s_pool = self.peer.storage.root.pool()
        pools = s_pool.list()
        #logger.debug('Checking health of Pools "%s" on Peer "%s"', pools, self.peer.hostname)
        ret = True
        for pool in pools:
            if not pool.is_healthy():
                logger.error('Pool "%s" on Peer "%s" is NOT healthy!', pool.name, self.peer.hostname)
                ret = False
        return ret


"""
Resource Monitor
"""


class ResourceMonitor(Component):
    def __init__(self, res):
        super(ResourceMonitor, self).__init__()
        self.res = res
        self._status = None

    def started(self, *args):
        pass

    @property
    def service(self):
        return self.res.local.service

    def resource_health_check(self):
        status = self.service.status()

        # If we have two secondary nodes become primary
        if status['role'] == 'Secondary' and status['remote_role'] == 'Secondary':
            logger.info('Taking over as Primary for Resource "%s" because someone has to. ' +
                        '(dual secondaries).', self.res.name)
            self.primary()

        # TODO check Connection state
        if status['connection_state'] != 'Connected':
            logger.error('Resource "%s" has a connection state of "%s", not "Connected"!',
                         self.res.name, status['connection_state'])

        # TODO check UpToDate
        if status['disk_state'] != 'UpToDate':
            logger.error('Resource "%s" has a disk state of "%s", not "UpToDate"!',
                         self.res.name, status['disk_state'])

        self._status = status.copy()

    def get_primary(self):
        if self._status['role'] == 'Primary':
            return self.res.local
        elif self._status['remote_role'] == 'Primary':
            return self.res.remote

    def write_config(self, adjust=True):
        self.service.write_config()
        if adjust:
            self.service.adjust()
        return True

    def primary(self):
        self.service.primary()

    def secondary(self):
        self.service.secondary()


"""
Target Monitor
"""


class TargetWriteConfig(Event):
    """Write Target Config"""


class TargetReloadConfig(Event):
    """Write Target Config"""


# TODO
class TargetMonitor(Component):
    def __init__(self):
        super(TargetMonitor, self).__init__()

    #def peer_heartbeat(self):
    #    pass

    def target_write_config(self):
        self.service.scst_write_config()
        self.fire(TargetReloadConfig())

    def target_reload_config(self):
        self.service.scst_reload_config()

    def target_scst_write_config_and_reload(self):
        self.service.scst_write_config()
        self.service.scst_clear_config()
        self.service.scst_reload_config()


"""
Main
"""


def set_proctitle(append=None):
    title = 'SolarSan Monitor'
    if append:
        title += ': %s' % append
    setproctitle('[%s]' % title)


def main():
    set_proctitle('Starting')
    try:
        #(Discovery()).run()
        #(Discovery() + Debugger()).run()
        (Monitor() + Debugger()).run()
    except (SystemExit, KeyboardInterrupt):
        raise


if __name__ == '__main__':
    main()
