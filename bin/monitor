#!/usr/bin/env python

from solarsan.core import logger
#from solarsan.utils.cache import cached_property
#from solarsan import conf
#from solarsan.exceptions import FormattedException
from setproctitle import setproctitle
from circuits import Component, Event, Debugger, Timer, handler
#from circuits.tools import inspect
from storage.drbd import DrbdResource, drbd_overview_parser
from cluster.models import Peer
from target.models import iSCSITarget
#import target.scst
from solarsan.monitor.udev import UDev
from solarsan.monitor.discovery import Discovery
from ha.models import ActivePassiveIP
#from configure.models import Nic


"""
Monitor
"""


class MonitorStatusUpdate(Event):
    """Monitor Status Update"""


class Monitor(Component):
    def __init__(self):
        super(Monitor, self).__init__()
        logger.info('Monitor starting..')

        Discovery().register(self)
        PeerManager().register(self)
        ResourceManager().register(self)
        TargetManager().register(self)
        DeviceManager().register(self)
        FailoverIPManager().register(self)

    def started(self, *args):
        self.fire(MonitorStatusUpdate('Started'))
        logger.info('Monitor started.')

    @handler('monitor_status_update', channel='*')
    def status_update(self, append=None):
        set_proctitle(append)

    #def peer_heartbeat(self, *args, **kwargs):
    #    logger.info('Peer Online @ Monitor')


"""
Peer Manager
"""


class PeerHeartbeat(Event):
    """Remote PeerHeartbeat"""
    #complete = True


class PeerHeartbeatTimeout(Event):
    """Remote PeerHeartbeat Timeout"""
    #complete = True


class PeerPoolHealthCheck(Event):
    """Checks Pool Health"""


class PeerManager(Component):
    heartbeat_every = 5.0
    pool_health_every = 10.0

    def __init__(self):
        super(PeerManager, self).__init__()
        self.peers = {}
        self.monitors = {}
        Timer(self.heartbeat_every, PeerHeartbeat(), persist=True).register(self)
        Timer(self.pool_health_every, PeerPoolHealthCheck(), persist=True).register(self)

    @handler('started', channel='*')
    def _on_started(self, *args):
        for peer in Peer.objects.all():
            self.add_peer(peer)

    @handler('peer_discovered', channel='*')
    def _on_peer_discovered(self, peer, created=None):
        self.add_peer(peer)

    def add_peer(self, peer):
        if peer.uuid in self.peers:
            return
        logger.info("Monitoring Peer '%s'.", peer.hostname)
        self.peers[peer.uuid] = peer
        self.monitors[peer.uuid] = PeerMonitor(peer).register(self)


"""
Resource Manager
"""


class ResourceHealthCheck(Event):
    """Check Resource Health"""
    #complete = True


class ResourceManager(Component):
    res_health_check_every = 10.0

    def __init__(self):
        super(ResourceManager, self).__init__()
        self.resources = {}
        self.monitors = {}
        Timer(self.res_health_check_every, ResourceHealthCheck(), persist=True).register(self)

    @handler('started', channel='*')
    def _on_started(self, *args):
        self.do_all_resources()

    def do_all_resources(self):
        for res in DrbdResource.objects.all():
            self.add_res(res)

    def add_res(self, res):
        if res.name in self.resources:
            return
        logger.info("Monitoring Resource '%s'.", res.name)
        self.resources[res.name] = res
        self.monitors[res.name] = ResourceMonitor(res).register(self)


"""
Target Manager
"""


class TargetManager(Component):

    def __init__(self):
        super(TargetManager, self).__init__()
        #self.targets = {}
        self.monitors = {}

    def started(self, *args):
        for tgt in iSCSITarget.objects.all():
            #self.targets[tgt.name] = tgt
            self.monitors[tgt.name] = TargetMonitor(tgt).register(self)


class TargetStart(Event):
    """Start Target"""


class TargetStop(Event):
    """Stop Target"""


class TargetStarted(Event):
    """Start Target"""


class TargetStopped(Event):
    """Stop Target"""


class TargetMonitor(Component):
    def __init__(self, target):
        super(TargetMonitor, self).__init__()
        self.target = target

    # When a Peer that we split this target with failsover,
    # become primary for the target.
    # ^ This should already be handled
    #   by resourcemonitor, the exception is when monitor is started while peer
    #   is already dead. In that case, peer_failover will work.
    def peer_failover(self, peer):
        for dev in self.target.devices:
            if dev.remote.uuid != peer.uuid:
                continue

            self.fire(TargetStart(self.target))
            return True

    def resource_primary_post(self, res):
        for dev in self.target.devices:
            if dev.pk == res.pk:
                #logger.info('Target "%s" member Resource "%s" has become primary.', self.target.name, res.name)
                self.fire(TargetStart(self.target))

    def resource_secondary_pre(self, res):
        for dev in self.target.devices:
            if dev.pk == res.pk:
                logger.info('Target "%s" member Resource "%s" wants to become secondary. Trying to ' +
                            'deconfigure quickly enough so it can.', self.target.name, res.name)
                self.fire(TargetStop(self.target))
                #logger.warning('Target "%s" member Resource "%s" is trying to become secondary while ' +
                #               'being part of an active target.',
                #               self.target.name, res.name)

    def check_if_missing_devices(self):
        missing_devices = []
        for dev in self.target.devices:
            if not dev.role == 'Primary':
                missing_devices.append(dev.name)

        if missing_devices:
            logger.error('Target "%s" cannot start because some luns are not available: "%s".', self.target.name,
                         missing_devices)
            return True
        return False

    def target_start(self, target):
        if target.name != self.target.name:
            return
        if self.check_if_missing_devices():
            return
        logger.info('Target "%s" can now start!', self.target.name)
        self.target.start()
        self.fire(TargetStarted(self.target))

    def target_stop(self, target):
        if target.name != self.target.name:
            return
        logger.info('Target "%s" can now stop!', self.target.name)
        self.target.stop()
        self.fire(TargetStopped(self.target))


"""
Failover IP Manager
"""


class FailoverIPManager(Component):
    def __init__(self):
        super(FailoverIPManager, self).__init__()
        self.ips = {}
        self.monitors = {}

    def started(self, *args):
        for ip in ActivePassiveIP.objects.all():
            self.ips[ip.name] = ip
            self.monitors[ip.name] = FailoverIPMonitor(ip).register(self)


"""
Failover IP Monitor
"""


class ActiveIP(Event):
    """Start FailoverIP"""


class PassiveIP(Event):
    """Stop FailoverIP"""


class FailoverIPMonitor(Component):
    def __init__(self, ip):
        super(FailoverIPMonitor, self).__init__()
        self.ip = ip
        if ip.is_active:
            logger.warn('Floating IP "%s" is currently active upon startup.', self.ip.iface_name)
            # May want to just disable on startup..
            #self.ip.ifdown()
            if ip.is_peer_active():
                logger.error('Floating IP "%s" appears to be up in both locations?! Deactivating..', self.ip.iface_name)
                self.ip.ifdown()

    ## peer_offline hits faster, but we want the IP comeup to be the very last
    ## thing to be done throughout a failover.
    #@handler('peer_failover', channel='*')
    #def _on_peer_failover(self, peer):
    #    if peer.uuid != self.ip.peer.uuid:
    #        return
    #    logger.error('Failing over floating IP "%s" for offline Peer "%s".',
    #                 self.ip.iface_name, self.ip.peer.hostname)
    #    self.fire(ActiveIP(self.ip))

    @handler('target_started', channel='*')
    def _on_target_started(self, target):
        if target.floating_ip.pk != self.ip.pk:
            return
        self.fire(ActiveIP(self.ip))

    @handler('target_stopping', channel='*')
    def _on_target_stopping(self, target):
        if target.floating_ip.pk != self.ip.pk:
            return
        self.fire(PassiveIP(self.ip))

    @handler('active_ip', channel='*')
    def _on_active_ip(self, ip):
        if ip.pk != self.ip.pk:
            return
        self.ifup()

    @handler('passive_ip', channel='*')
    def _on_passive_ip(self, ip):
        if ip.pk != self.ip.pk:
            return
        self.ifdown()

    def ifup(self):
        if self.ip.is_active:
            return
        logger.info('Floating IP "%s" is being brought up.', self.ip.iface_name)
        self.ip.ifup()

    def ifdown(self):
        if not self.ip.is_active:
            return
        logger.info('Floating IP "%s" is being brought down.', self.ip.iface_name)
        self.ip.ifdown()


"""
Device Manager
"""


class DeviceManager(Component):
    def __init__(self):
        super(DeviceManager, self).__init__()

        UDev().register(self)

    #def device_add(self, device):
    #    pass

    #def device_remove(self, device):
    #    pass


"""
Peer Monitor
"""


class PeerOnline(Event):
    """Peer Online Event"""


class PeerOffline(Event):
    """Peer Offline Event"""


class PeerFailover(Event):
    """Peer Failover Event"""


class PeerPoolNotHealthy(Event):
    """Pool is Not Healthy"""


class PeerStillOffline(Event):
    """Peer is *still* offline"""


class PeerMonitor(Component):
    heartbeat_timeout_after = 2

    def __init__(self, peer):
        super(PeerMonitor, self).__init__()
        self.peer = peer
        self._heartbeat_timeout_count = None

        if self.peer.is_offline:
            logger.warning('Peer "%s" is already marked as offline. Marking online to ensure this is ' +
                           'still true.', self.peer.hostname)
            self.mark_online(startup=True)

    @handler('peer_heartbeat', channel='*')
    def _on_peer_heartbeat(self):
        # This is done so the first try is always attempted, even if the Peer
        # is offline.
        if not self._heartbeat_timeout_count:
            self._heartbeat_timeout_count = 0
        elif self.peer.is_offline:
            #logger.debug('PeerHeartbeat is not even being attempted as Peer "%s" is marked offline.', self.peer.hostname)
            return

        ret = self.ping()
        if ret:
            self._heartbeat_timeout_count = 0
        else:
            self._heartbeat_timeout_count += 1
            if self._heartbeat_timeout_count >= self.heartbeat_timeout_after:
                self.fire(PeerOffline(self.peer))
        return ret

    def ping(self):
        try:
            storage = self.peer.get_service('storage', default=None)
            storage.ping()
            #logger.debug('PeerHeartbeat back from Peer "%s"', self.peer.hostname)
            return True
        except:
            logger.error('Did not receive heartbeat for Peer "%s"', self.peer.hostname)
            return False

    @handler('peer_online', channel='*')
    def _on_peer_online(self, peer):
        if peer.uuid != self.peer.uuid:
            return
        if self.peer.is_offline:
            self.mark_online()

    @handler('peer_offline', channel='*')
    def _on_peer_offline(self, peer):
        if peer.uuid != self.peer.uuid:
            return
        if not self.peer.is_offline:
            self.mark_offline()

    def mark_online(self, startup=False):
        if not startup:
            logger.warning('Peer "%s" is ONLINE!', self.peer.hostname)
        self.peer.is_offline = False
        self.peer.save()
        if hasattr(self, '_offline_timer'):
            self._offline_timer.unregister()
            delattr(self, '_offline_timer')
        self._heartbeat_timeout_count = 0

    def mark_offline(self):
        logger.error('Peer "%s" is OFFLINE!', self.peer.hostname)
        self.peer.is_offline = True
        self.peer.save()
        self._offline_timer = Timer(60.0, PeerStillOffline(self.peer), persist=True).register(self)
        self.fire(PeerFailover(self.peer))

    # nagnagnagnagnagnag
    @handler('peer_still_offline', channel='*')
    def _on_peer_still_offline(self, peer):
        if peer.uuid != self.peer.uuid:
            return
        logger.warning("Peer '%s' is STILL offline!", peer.hostname)

    @handler('peer_discovered', channel='*')
    def _on_peer_discovered(self, peer, created=False):
        if peer.uuid != self.peer.uuid:
            return
        if self.peer.is_offline:
            self.fire(PeerOnline(self.peer))

    @handler('peer_pool_health_check', channel='*')
    def _on_peer_pool_health_check(self):
        if self.peer.is_offline:
            return
        try:
            storage = self.peer.get_service('storage', default=None)
            Pool = storage.root.pool()
        except AttributeError:
            logger.error('Could not connect to Peer "%s". Is it offline and we don\'t know it yet?',
                         self.peer.hostname)
            return
        pools = Pool.list()
        #logger.debug('Checking health of Pools "%s" on Peer "%s"', pools, self.peer.hostname)
        ret = True
        for pool in pools:
            if not pool.is_healthy():
                self.fire(PeerPoolNotHealthy(self.peer, pool.name))
                ret = False
        return ret

    @handler('peer_pool_not_healthy', channel='*')
    def _on_peer_pool_not_healthy(self, peer, pool):
        if peer.uuid != self.peer.uuid:
            return
        logger.error('Pool "%s" on Peer "%s" is NOT healthy!', pool, self.peer.hostname)

        # TODO If Pool has remote disks, and those are the only ones that are
        # missing, try a 'zpool clear'


"""
Resource Monitor
"""


class ResourcePrimaryPre(Event):
    """Promote to Primary"""


class ResourcePrimary(Event):
    """Promote to Primary"""


class ResourcePrimaryPost(Event):
    """Promote to Primary"""


class ResourceSecondaryPre(Event):
    """Demote to Secondary"""


class ResourceSecondary(Event):
    """Demote to Secondary"""


class ResourceSecondaryPost(Event):
    """Demote to Secondary"""


class ResourceConnectionStateChange(Event):
    """Resource Connection State Change"""


class ResourceDiskStateChange(Event):
    """Resource Disk State Change"""


class ResourceRoleChange(Event):
    """Resource Connection State Change"""
    #success = True
    #complete = True


class ResourceRemoteDiskStateChange(Event):
    """Resource Remote Disk State Change"""


class ResourceRemoteRoleChange(Event):
    """Resource Remote Role State Change"""


class ResourceMonitor(Component):
    def __init__(self, res):
        super(ResourceMonitor, self).__init__()
        self.res = res
        self.status()

    @handler('peer_failover', channel='*')
    def _on_peer_failover(self, peer):
        if peer.uuid != self.res.remote.peer.uuid:
            return
        logger.error('Failing over Peer "%s" for Resource "%s".',
                     self.res.remote.hostname, self.res.name)

        ## TODO Take over the volumes, write out any targets, enable any HA ips, etc
        if self.res.role != 'Primary':
            self.fire(ResourcePrimary(self.res))

    @handler('peer_pool_not_healthy', channel='*')
    def _on_peer_pool_not_healthy(self, peer, pool):
        if peer.uuid == self.res.local.peer.uuid and pool == self.res.local.pool:
            #logger.error('Pool "%s" for Resource "%s" is not healthy.',
            #             pool, self.res.name)
            pass

            ## TODO Remove any HA IPs for this resource (handled by the target,
            # nothing here)
            #   - But what if the HA IPs are used for others? HA IPs must be
            #   attached to targets, and only whole targets can go up or down.
            # TODO Remove any targets for this resource
            #   - But what if the targets are used for other resources? Again,
            #   must do a whole target at a time, up or down.
            # TODO Secondary ourselves
            #self.fire(ResourceSecondary(self.res))

        elif peer.uuid == self.res.remote.uuid and pool == self.res.remote.pool:
            #logger.error('Pool "%s" on Remote "%s" for Resource "%s" is not healthy.',
            #             pool, self.res.remote.hostname, self.res.name)
            pass

            # TODO Primary ourselves
            # TODO Add any targets for this resource
            #   - But what if the targets are used for other resources? Again,
            #   must do a whole target at a time, up or down.
            # TODO Add any HA IPs for that target (handled by the target,
            # nothing here)
            #   - But what if the HA IPs are used for others? HA IPs must be
            #   attached to targets, and only whole targets can go up or down.
            #self.fire(ResourcePrimary(self.res))

    @property
    def service(self):
        return self.res.local.service

    #def get_service(self):
    #    return self.res.local.service

    """
    Status Tracking
    """

    def status(self):
        ret = drbd_overview_parser(self.res.name)
        if not ret:
            return ret

        events = []
        if ret['connection_state'] != self.res.connection_state:
            events.append(ResourceConnectionStateChange(self.res, ret['connection_state']))
        if ret['disk_state'] != self.res.disk_state:
            events.append(ResourceDiskStateChange(self.res, ret['disk_state']))
        if ret['role'] != self.res.role:
            events.append(ResourceRoleChange(self.res, ret['role']))
        if ret['remote_disk_state'] != self.res.remote_disk_state:
            events.append(ResourceRemoteDiskStateChange(self.res, ret['remote_disk_state']))
        if ret['remote_role'] != self.res.remote_role:
            events.append(ResourceRemoteRoleChange(self.res, ret['remote_role']))

        keys = ['connection_state', 'disk_state', 'role']
        keys += ['remote_' + x for x in keys[1:]]

        for k, v in ret.iteritems():
            if k in keys:
                setattr(self.res, k, v)
        self.res.save()

        for event in events:
            self.fire(event)

        return ret

    def resource_connection_state_change(self, res, cstate):
        if self.res.name != res.name:
            return

        if cstate == 'Connected':
            """Connected. A DRBD connection has been established, data mirroring is now active. This is the
            normal state"""
            logger.error("Resource '%s' is %s!", self.res.name, cstate)

        elif cstate == 'WFConnection':
            """WFConnection. This node is waiting until the peer node becomes visible on the network."""
            logger.error("Resource '%s' is %s!", self.res.name, cstate)
            #if self.res.role != 'Primary':
            #    self.fire(ResourcePrimary(self.res))

        elif cstate == 'StandAlone':
            """StandAlone. No network configuration available. The resource has not yet been connected, or
            has been administratively disconnected (using drbdadm disconnect), or has dropped its connection
            due to failed authentication or split brain."""
            logger.error("Resource '%s' is %s!", self.res.name, cstate)
            #if self.res.role != 'Primary':
            #    self.fire(ResourcePrimary(self.res))

        elif cstate == 'SyncSource':
            """SyncSource. Synchronization is currently running, with the local node being the source of
            synchronization"""
            logger.error("Resource '%s' is %s!", self.res.name, cstate)
            #if self.res.role != 'Primary':
            #    self.fire(ResourcePrimary(self.res))

        elif cstate == 'PausedSyncS':
            """PausedSyncS. The local node is the source of an ongoing synchronization, but synchronization
            is currently paused. This may be due to a dependency on the completion of another synchronization
            process, or due to synchronization having been manually interrupted by drbdadm pause-sync."""
            logger.error("Resource '%s' is %s!", self.res.name, cstate)

        elif cstate == 'SyncTarget':
            """SyncTarget. Synchronization is currently running, with the local node being the target of
            synchronization."""
            logger.error("Resource '%s' is %s!", self.res.name, cstate)

        elif cstate == 'PausedSyncT':
            """PausedSyncT. The local node is the target of an ongoing synchronization, but synchronization
            is currently paused. This may be due to a dependency on the completion of another synchronization
            process, or due to synchronization having been manually interrupted by drbdadm pause-sync."""
            logger.error("Resource '%s' is %s!", self.res.name, cstate)

        elif cstate == 'VerifyS':
            """VerifyS. On-line device verification is currently running, with the local node being the source
            of verification."""
            logger.error("Resource '%s' is %s!", self.res.name, cstate)

        elif cstate == 'VerifyT':
            """VerifyT. On-line device verification is currently running, with the local node being the target
            of verification."""
            logger.error("Resource '%s' is %s!", self.res.name, cstate)

        else:
            raise Exception('Resource "%s" has an unknown connection state of "%s"!', cstate)

    def resource_disk_state_change(self, res, dstate):
        if self.res.name != res.name:
            return

        if dstate == 'UpToDate':
            """UpToDate. Consistent, up-to-date state of the data. This is the normal state."""
            logger.error("Resource '%s' is %s!", self.res.name, dstate)

        elif dstate == 'Diskless':
            """Diskless. No local block device has been assigned to the DRBD driver. This may mean that the
            resource has never attached to its backing device, that it has been manually detached using
            drbdadm detach, or that it automatically detached after a lower-level I/O error."""
            logger.error("Resource '%s' is %s!", self.res.name, dstate)

        elif dstate == 'Inconsistent':
            """Inconsistent. The data is inconsistent. This status occurs immediately upon creation of a new
            resource, on both nodes (before the initial full sync). Also, this status is found in one node
            (the synchronization target) during synchronization."""
            logger.error("Resource '%s' is %s!", self.res.name, dstate)

        elif dstate == 'Outdated':
            """Outdated. Resource data is consistent, but outdated."""
            logger.error("Resource '%s' is %s!", self.res.name, dstate)

        elif dstate == 'DUnknown':
            """DUnknown. This state is used for the peer disk if no network connection is available."""
            logger.error("Resource '%s' is %s!", self.res.name, dstate)

        elif dstate == 'Consistent':
            """Consistent. Consistent data of a node without connection. When the connection is established,
            it is decided whether the data is UpToDate or Outdated."""
            logger.error("Resource '%s' is %s!", self.res.name, dstate)

        else:
            raise Exception('Resource "%s" has an unknown disk state of "%s"!', dstate)

    def resource_role_change(self, res, role):
        if self.res.name != res.name:
            return

        if role == 'Primary':
            """Primary. The resource is currently in the primary role, and may be read from and written to.
            This role only occurs on one of the two nodes, unless dual-primary mode is enabled."""
            logger.error("Resource '%s' is %s!", self.res.name, role)

        elif role == 'Secondary':
            """Secondary. The resource is currently in the secondary role. It normally receives updates from
            its peer (unless running in disconnected mode), but may neither be read from nor written to.
            This role may occur on one or both nodes."""
            logger.error("Resource '%s' is %s!", self.res.name, role)

        elif role == 'Unknown':
            """Unknown. The resource's role is currently unknown. The local resource role never has this
            status. It is only displayed for the peer's resource role, and only in disconnected mode."""
            logger.error("Resource '%s' is %s!", self.res.name, role)

        else:
            raise Exception('Resource "%s" has an unknown role of "%s"!', role)

    def resource_remote_disk_state_change(self, res, dstate):
        if self.res.name != res.name:
            return

    def resource_remote_role_change(self, res, remote_role):
        if self.res.name != res.name:
            return

    """
    Health Check
    """

    def resource_health_check(self):
        self.status()

        # If we have two secondary nodes become primary
        if self.res.role == 'Secondary' and self.res.remote_role == 'Secondary':
            logger.info('Taking over as Primary for Resource "%s" because someone has to. ' +
                        '(dual secondaries).', self.res.name)
            self.fire(ResourcePrimary(self.res))

    """
    Role Switching
    """

    @handler('resource_primary', channel='*')
    def _on_res_primary(self, res):
        if res.name != self.res.name:
            return
        self.status()
        if self.res.role == 'Primary':
            return

        logger.warning('Promoting self to Primary for Resource "%s".', self.res.name)

        if self.res.disk_state != 'UpToDate':
            logger.error('Cannot promote self to Primary for Resource "%s", ' +
                         'as disk state is not UpToDate.', self.res.name)
            return

        self.primary()

    @handler('resource_secondary_success', channel='*')
    def _on_res_secondary(self, res, *values):
        if res.name != self.res.name:
            return
        self.status()
        if self.res.role == 'Secondary':
            return
        logger.warning('Demoting self to Secondary for Resource "%s".', self.res.name)
        self.secondary()

    def primary(self):
        self.fire(ResourcePrimaryPre(self.res))
        self.service.primary()
        self.status()
        self.fire(ResourcePrimaryPost(self.res))

    def secondary(self):
        self.fire(ResourceSecondaryPre(self.res))
        self.service.secondary()
        self.status()
        self.fire(ResourceSecondaryPost(self.res))

    """
    TODO
    """

    #def get_primary(self):
    #    if self._status['role'] == 'Primary':
    #        return self.res.local
    #    elif self._status['remote_role'] == 'Primary':
    #        return self.res.remote

    def write_config(self, adjust=True):
        self.service.write_config()
        if adjust:
            self.service.adjust()
        return True


"""
Main
"""


def set_proctitle(append=None):
    title = 'SolarSan Monitor'
    if append:
        title += ': %s' % append
    setproctitle('[%s]' % title)


def main():
    set_proctitle('Starting')
    try:
        #(Discovery()).run()
        #(Discovery() + Debugger()).run()
        (Monitor() + Debugger()).run()
    except (SystemExit, KeyboardInterrupt):
        raise


if __name__ == '__main__':
    main()
